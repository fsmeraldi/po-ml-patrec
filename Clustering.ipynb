{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli algoritmi di clustering si occupano della selezione e del raggruppamento di un insieme di dati in sottoinsiemi caratterizzati da \n",
    "* omogeneita' all'interno di ciascun sottoinsieme\n",
    "* diversita' da un sottoinsieme all'altro.\n",
    "\n",
    "Omogeneita' e diversita' sono concetti da specificare in maniera quantitativa.\n",
    "\n",
    "\n",
    "## Tipi di algoritmi\n",
    "\n",
    "La costruzione dei sottoinsiemi puo' effettuarsi tramite:\n",
    "* *aggregazione* dei singoli dati (bottom-up): \n",
    "    * es. single linkage clustering\n",
    "* *divisione* dell'insieme dei dati (top-down): \n",
    "    * es. cut clustering (sui grafi)\n",
    "\n",
    "Dal punto di vista del risultato, possiamo parlarare di\n",
    "* *clustering partizionale*, che risulta in una partizione dei dati \n",
    "    * es. k-Means\n",
    "* *clustering gerarchico*, che porta a una gerarchia di partizioni (un albero)\n",
    "    * es. single-linkage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distanza e spazi metrici\n",
    "\n",
    "La matematica ci viene in aiuto con la nozione di distanza. Dato un insieme $X$, una distanza e' una funzione\n",
    "\n",
    "$d(X \\times X ) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "che soddisfa le seguenti proprieta' per ogni $x,y, z \\in  X$:\n",
    "1. $d(x,y) \\geq 0$ (positivita')\n",
    "2. $d(x,y)=0 \\iff x=y$ (identita' degli indiscernibili)\n",
    "3. $d(x,y)=d(y,x)$ (simmetria) \n",
    "4. $d(x,y)\\leq d(x,z) +d(z,y)$ (diseguaglianza triangolare)\n",
    "\n",
    "La coppia $(X,d)$ costituisce uno spazio metrico. La maggior parte degli algoritmi di clustering richiede almeno uno spazio metrico; vari algoritmi richiedono strutture piu' ricche (spazi vettoriali).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio 1: clustering delle citta' italiane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensiamo alle citta' come punti su un piano cartesiano\n",
    "\n",
    "Distanza euclidea ($\\simeq$ distanza geografica):\n",
    "\n",
    "$d_E(\\mathbb{R}^2 \\times \\mathbb{R}^2 ) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "quindi, per due punti $\\vec{u}$, $\\vec{v}$ sul piano:\n",
    "\n",
    "$d(\\vec{v},\\vec{w})=\\sqrt{(v_x-w_x)^2+(v_y-w_y)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![distanze](https://famigliainfuga.com/wp-content/uploads/2020/06/distanze-chilometriche-google-maps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio 2: clustering delle pizze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| pizza      | pomodoro | mozzarella | acciughe | capperi | aglio |\n",
    "|------------|----------|------------|----------|---------|-------|\n",
    "| Margherita |    1     |         1  |       0  |      0  |    0  |\n",
    "| Marinara   |     1    |         0  |       0  |      0  |    1  |\n",
    "| Napoli     |     1    |         1  |       1  |      1  |    0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X=\\{0,1\\}^5$\n",
    "\n",
    "Distanza di Hamming: numero di ingredienti presenti solo in una delle due pizze\n",
    "    \n",
    "$d_H(p, q)= \\sum_i |x_i - y_i|$\n",
    "\n",
    "(facilmente estensibile a descrittori di tipo Booleano non alimentare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio 3: clustering di sequenze di DNA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$: spazio delle stringhe con elementi in $\\{A,T,C,G\\}$\n",
    "\n",
    "Esempio:\n",
    "s1=`CATACG`\n",
    "s2=`GATTACA`\n",
    "\n",
    "[Distanza di Levenshtein](https://it.wikipedia.org/wiki/Distanza_di_Levenshtein): *minimo* numero di operazioni di editing (sostituzione, inserimento, cancellazione) necessarie a trasformare la stringa s1 nella stringa s2\n",
    "\n",
    "CATACG ->  `G`ATACG -> GAT`T`ACG -> GATTAC`A`\n",
    "\n",
    "$d_L(s1, s2)=3$\n",
    "\n",
    "Il calcolo della distanza di Levenshtein e' un problema di progammazione dinamica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La scelta dei descrittori e della metrica e' importante!\n",
    "\n",
    "Ha senso fare un clustering delle citta' basato sulla distanza geografica?\n",
    "\n",
    "* Se cerchiamo di ottimizzare la posizione dei depositi per una serie di punti vendita, probabilmente si'\n",
    "\n",
    "Se abbiamo altri scopi, potremmo considerare:\n",
    "\n",
    "* Numero degli abitanti\n",
    "* Distribuzione degli abitanti per fasce d'eta'\n",
    "* Distribuzione degli abitanti per fasce di reddito\n",
    "* Distribuzione dei lavoratori per settore economico\n",
    "\n",
    "Un linguista computazionale potrebbe voler fare un clustering delle pizze basato sul loro nome e sulla distanza di Levenshtein!\n",
    "\n",
    "---------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'algoritmo K-Means\n",
    "\n",
    "\n",
    "* Uno degli algoritmi piu' popolari\n",
    "* Molto usato in computer graphics/computer vision\n",
    "* Richiede uno spazio vettoriale\n",
    "* Usa la distanza euclidea (e' generalizzabile a [divergenze di Bregman](https://www.jmlr.org/papers/volume6/banerjee05b/banerjee05b.pdf))\n",
    "* Risolve un problema di ottimizzazione\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means come compressione con perdita d'informazione\n",
    "\n",
    "Supponiamo di dover trasmettere le coordinate di questi punti usando solo due bit per punto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_1.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobbiamo accettare la perdita d'informazione; come penalita', prendiamo la somma dei quadrati delle distanze tra i punti originali e quelli trasmessi. \n",
    "\n",
    "**Come approssimare i punti in maniera ottimale?**\n",
    "\n",
    "**IDEA No. 1**: Dividiamo il piano in quadranti. Per ogni punto, trasmettiamo il numero del quadrante (2 bit). Ricostruiamo i dati come centro del quadrante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_2.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA No. 2:** Per ogni quadrante, approssimiamo i punti col centroide dei dati in quel quadrante "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_3.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA No. 3**: Perche' definire le partizioni a priori, e non in base ai dati? Possiamo limitarci a specificare solo il numero delle partizioni?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'algoritmo\n",
    "\n",
    "**PASSO 1**: Chiediamo all'utente quanti cluster vuole (esempio: k=5)\n",
    "\n",
    "**PASSO 2**: Scegliamo k centri a caso (esempio: k punti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_4.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PASSO 3:** Ogni dato trova il centro al quale e' piu' vicino (ogni centro reclama i \"suoi\" punti). Questo di fatto \"crea\" le partizioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_5.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PASSO 4**: Ogni centro si sposta sul centroide dei punti nella sua partizione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_6.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo ci riporta nella situazione del passo 2, ma con nuovi centri:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmeans_7.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...e possiamo iterare fino alla convergenza (non tutti i passi sono visualizzati; ignorate i quadrati)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/kmeans_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Per la presentazione di k-Means come un algoritmo di compressione e le immagini, sono debitore al [Prof. Andrew Moore](http://www.cs.cmu.edu/~awm/tutorials.html), Carnegie Mellon University.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riassumendo l'algoritmo:\n",
    "\n",
    "* input: dati, numero di cluster k\n",
    "* inizializzazione: scelta di k centri a caso\n",
    "* ripetere fino a convergenza:\n",
    "    * ogni punto sceglie il centro piu' vicino\n",
    "    * ogni centro si sposta sul centroide dei suoi punti    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domande:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cosa ottimizza k-means? \n",
    "* L'algoritmo converge?\n",
    "* A cosa converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means minimizza l'errore di approssimazione**\n",
    "\n",
    "$E=\\sum_i \\left(\\vec{x_i}- \\vec{c}_{p(i)}\\right)^2$,\n",
    "\n",
    "dove  $p(i)$ e' l'indice del centroide (o della partizione) assegnata dall'algoritmo al punto $\\vec{x_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perche' $E$ sia minimo, devono verificarsi due condizioni:\n",
    "\n",
    "**Condizione 1:** ogni punto $\\vec{x_i}$ e' assegnato al centroide $\\vec{c}_j$ a lui piu' vicino. \n",
    "\n",
    "Questo e' evidente: se cosi' non fosse, basterebbe riassegnare $\\vec{x_i}$ per ridurre $E$.\n",
    "\n",
    "**Condizione 2:** la derivata parziale dell'errore rispetto alla coordinate dei centri deve essere zero. Con $N$ numero dei dati e $k$ numero dei cluster:\n",
    "\n",
    "$E=\\sum_{i=1}^N\\left(\\vec{x_i}-\\vec{c}_{p(i)}\\right)^2=\\sum_{j=1}^k \\sum_{i\\in p^{-1}(j)}\\left(\\vec{x_i}-\\vec{c}_j\\right)^2$\n",
    "\n",
    "e quindi, con leggero abuso di notazione:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial{\\vec{c}_j}}=\n",
    "\\frac{\\partial}{\\partial{\\vec{c}_j}}\\sum_{i\\in p^{-1}(j)}\\left(\\vec{x_i}-\\vec{c}_j\\right)^2=\n",
    "-2\\sum_{i\\in p^{-1}(j)}\\left(\\vec{x_i}-\\vec{c}_j\\right)=0\\qquad \\mathrm{nel\\,minimo}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ovvero\n",
    "\\begin{equation}\n",
    "\\vec{c_j}=\\frac{1}{\\left| p^{-1}(j)\\right|} \\sum_{i\\in p^{-1}(j)}\\vec{x_i}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riassumendo:\n",
    "    \n",
    "**Condizione 1:** Ogni $\\vec{x}_i$ deve essere assegnato al centro piu' vicino\n",
    "\n",
    "**Condizione 2:** Ogni centro deve essere uguale alla media dei suoi punti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se siamo lontani dal minimo, possiamo provare a cambiare le cose in modo da imporre una delle due condizioni.\n",
    "\n",
    "Imporre la stessa condizione due volte non porta a niente, ma alternare tra le due puo' essere utile - e questo e' k-means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questo procedimento ha termine?\n",
    "\n",
    "* c'e' un numero finito di modi di assegnare i punti ai centri\n",
    "* ogni volta, imporre una condizione ci porta ad una configurazione con $E$ piu' basso\n",
    "* ne segue che l'algoritmo esaurisce ad un certo punto le configurazioni accessibili e quindi converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questioni aperte (per la prossima lezione):\n",
    "\n",
    "* Il minimo al quale k-means converge e' un minimo locale\n",
    "* Criteri per la scelta di k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
